{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# XOR without numpy\n",
        "\n",
        "This program implements a simple neural network with one hidden layer that is used to learn and predict the XOR function. Let's break down the code step by step:\n",
        "\n",
        "The script uses stochastic gradient descent to train the neural network, one row of data at a time, so there is no need for the matrix transposition that would be required for mini-batches. The loss function is the root mean square error.\n",
        "\n",
        "This is the simplest script, implementation:\n",
        "\n",
        "[![nn.png](https://i.postimg.cc/Hxf8jKqQ/nn.png)](https://postimg.cc/TLJ34kYw)"
      ],
      "metadata": {
        "id": "5fOnOGTZrklE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, a neural network is just a bunch of imprecisely written variables."
      ],
      "metadata": {
        "id": "yAhXXJTOszU7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialization"
      ],
      "metadata": {
        "id": "KNpws9o_xN0P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We import random and mathematical modules.\n",
        "import random\n",
        "import math\n",
        "\n",
        "\n",
        "# We set the variability for initializing the weights.\n",
        "VARIANCE_W = 0.5\n",
        "\n",
        "# We initialize the weights (w11, w21, w12, w22, w13, w23) and shifts (b1, b2, b3) for the neurons of the hidden layer.\n",
        "w11 = random.uniform(-VARIANCE_W,VARIANCE_W)\n",
        "w21 = random.uniform(-VARIANCE_W,VARIANCE_W)\n",
        "b1 = 0\n",
        "\n",
        "w12 = random.uniform(-VARIANCE_W,VARIANCE_W)\n",
        "w22 = random.uniform(-VARIANCE_W,VARIANCE_W)\n",
        "b2 = 0\n",
        "\n",
        "w13 = random.uniform(-VARIANCE_W,VARIANCE_W)\n",
        "w23 = random.uniform(-VARIANCE_W,VARIANCE_W)\n",
        "b3 = 0\n",
        "\n",
        "# Initialize weights (o1, o2, o3) and offset (ob) for the output neuron.\n",
        "o1 = random.uniform(-VARIANCE_W,VARIANCE_W)\n",
        "o2 = random.uniform(-VARIANCE_W,VARIANCE_W)\n",
        "o3 = random.uniform(-VARIANCE_W,VARIANCE_W)\n",
        "ob = 0"
      ],
      "metadata": {
        "id": "3R4Np8CLmYCT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Variability** (variativnist): is a general term that means the degree of spread or changeability of something. In the context of machine learning, it describes how much the model's predictions vary depending on the training data set.\n",
        "**Sample Dependence** (zalezhnist vid vibirky): A more technical term that emphasizes that the variability arises because the model is too sensitive to the particular training data set it is given.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Offset** (ob) for an output neuron in an artificial neural network is a constant that is added to the input signal of the neuron before the activation function is applied.\n",
        "\n",
        "It plays an important role in the work of the neuron, performing the following functions:\n",
        "\n",
        "\n",
        "1. Baseline output: A shift can be used to base a neuron's output, which can be useful for some types of problems, such as classification problems where one class may have a higher probability than others.\n",
        "2. Improving learning performance: Bias can help improve the learning performance of a neuron by making it more robust to noise in the data and making it easier for it to learn complex features.\n",
        "3. Shifting the Decision Boundary: Shifting can be used to shift the decision boundary of a neuron, which can be useful for some types of problems, such as tuning problems where the neuron's sensitivity to changes in input data needs to be tuned.\n",
        "\n",
        "It is important to note that the offset value is usually determined during neural network training. This can be done using optimization algorithms such as gradient descent that automatically adjust the offset value to minimize the neuron's error.\n",
        "\n",
        "In addition to the above functions, the shift can also be used for other purposes, depending on the specific task and neural network architecture."
      ],
      "metadata": {
        "id": "WU7JeTJlnmJa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Activation function"
      ],
      "metadata": {
        "id": "TUVVq9m2x9tt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This is the activation function used to introduce nonlinearity.\n",
        "def sigmoid(x):\n",
        "    return 1.0 / (1.0 + math.exp(-x))\n",
        "\n",
        "# The derivative of the sigmoid function used for backpropagation.\n",
        "def sigmoid_prime(x): # X is already sigmoid.\n",
        "    return x * (1 - x)"
      ],
      "metadata": {
        "id": "mb-AnqCmzgz8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prediction function"
      ],
      "metadata": {
        "id": "B57Ec-sOzf_u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The prediction function calculates the output of the network from the inputs i1 and i2.\n",
        "def predict(i1,i2):\n",
        "    # It calculates the activations s1, s2 and s3 for hidden layer neurons using a sigmoid function.\n",
        "    s1 = w11 * i1 + w21 * i2 + b1\n",
        "    s1 = sigmoid(s1)\n",
        "    s2 = w12 * i1 + w22 * i2 + b2\n",
        "    s2 = sigmoid(s2)\n",
        "    s3 = w13 * i1 + w23 * i2 + b3\n",
        "    s3 = sigmoid(s3)\n",
        "\n",
        "    # It calculates the final result by combining the hidden layer activations with the output weights and applying a sigmoid function.\n",
        "    output = s1 * o1 + s2 * o2 + s3 * o3 + ob\n",
        "    output = sigmoid(output)\n",
        "\n",
        "    return output"
      ],
      "metadata": {
        "id": "0UglFmwSJOlT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Learning function"
      ],
      "metadata": {
        "id": "DjO2o6BZJQAf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1AC8ZwRXkC8_"
      },
      "outputs": [],
      "source": [
        "# The learning function updates the weights and biases based on the error between the predicted output and the target output.\n",
        "def learn(i1,i2,target, alpha=0.2):\n",
        "    # It performs forward propagation to calculate activations.\n",
        "    global w11,w21,b1,w12,w22,b2,w13,w23,b3\n",
        "    global o1,o2,o3,ob\n",
        "\n",
        "    s1 = w11 * i1 + w21 * i2 + b1\n",
        "    s1 = sigmoid(s1)\n",
        "    s2 = w12 * i1 + w22 * i2 + b2\n",
        "    s2 = sigmoid(s2)\n",
        "    s3 = w13 * i1 + w23 * i2 + b3\n",
        "    s3 = sigmoid(s3)\n",
        "\n",
        "    output = s1 * o1 + s2 * o2 + s3 * o3 + ob\n",
        "    output = sigmoid(output)\n",
        "\n",
        "    # It calculates the error and its derivative.\n",
        "    error = target - output\n",
        "    derror = error * sigmoid_prime(output)\n",
        "\n",
        "    ds1 = derror * o1 * sigmoid_prime(s1)\n",
        "    ds2 = derror * o2 * sigmoid_prime(s2)\n",
        "    ds3 = derror * o3 * sigmoid_prime(s3)\n",
        "\n",
        "    # It performs backpropagation to update the weights and biases using the alpha learning rate.\n",
        "    o1 += alpha * s1 * derror\n",
        "    o2 += alpha * s2 * derror\n",
        "    o3 += alpha * s3 * derror\n",
        "    ob += alpha * derror\n",
        "\n",
        "    w11 += alpha * i1 * ds1\n",
        "    w21 += alpha * i2 * ds1\n",
        "    b1 += alpha * ds1\n",
        "    w12 += alpha * i1 * ds2\n",
        "    w22 += alpha * i2 * ds2\n",
        "    b2 += alpha * ds2\n",
        "    w13 += alpha * i1 * ds3\n",
        "    w23 += alpha * i2 * ds3\n",
        "    b3 += alpha * ds3"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training and testing"
      ],
      "metadata": {
        "id": "s6Fgs2S3oLFS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The INPUTS and OUTPUTS lists represent input-output XOR pairs.\n",
        "INPUTS = [\n",
        "        [0,0],\n",
        "        [0,1],\n",
        "        [1,0],\n",
        "        [1,1]\n",
        "    ]\n",
        "\n",
        "OUTPUTS = [\n",
        "        [0],\n",
        "        [1],\n",
        "        [1],\n",
        "        [0]\n",
        "    ]\n",
        "\n",
        "# The training cycle lasts 10,000 epochs\n",
        "for epoch in range(1,10001):\n",
        "    indexes = [0,1,2,3]\n",
        "    random.shuffle(indexes)\n",
        "\n",
        "    # At each epoch, the input indices are shuffled and the network is trained on each input.\n",
        "    for j in indexes:\n",
        "        learn(INPUTS[j][0],INPUTS[j][1],OUTPUTS[j][0], alpha=0.2)\n",
        "\n",
        "    # The root mean square error is output every 1000 epochs.\n",
        "    if epoch%1000 == 0:\n",
        "        cost = 0\n",
        "        for j in range(4):\n",
        "            o = predict(INPUTS[j][0],INPUTS[j][1])\n",
        "            cost += (OUTPUTS[j][0] - o) ** 2\n",
        "        cost /= 4\n",
        "        print(\"epoch\", epoch, \"mean squared error:\", cost)\n",
        "\n",
        "# result output.\n",
        "for i in range(4):\n",
        "    result = predict(INPUTS[i][0],INPUTS[i][1])\n",
        "    print(\"for input\", INPUTS[i], \"expected\", OUTPUTS[i][0], \"predicted\", f\"{result:4.4}\", \"which is\", \"correct\" if round(result)==OUTPUTS[i][0] else \"incorrect\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OzmjA9IKoJFR",
        "outputId": "0227af9e-9b0f-4dce-82e9-9c10b27d5ec4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1000 mean squared error: 0.0008236728994545758\n",
            "epoch 2000 mean squared error: 0.000710268262431356\n",
            "epoch 3000 mean squared error: 0.0006234091972789838\n",
            "epoch 4000 mean squared error: 0.0005548692292482299\n",
            "epoch 5000 mean squared error: 0.0004994785500849843\n",
            "epoch 6000 mean squared error: 0.00045383151134952026\n",
            "epoch 7000 mean squared error: 0.00041559674737267835\n",
            "epoch 8000 mean squared error: 0.00038312648995585157\n",
            "epoch 9000 mean squared error: 0.00035522426269496725\n",
            "epoch 10000 mean squared error: 0.0003310009868735209\n",
            "for input [0, 0] expected 0 predicted 0.01849 which is correct\n",
            "for input [0, 1] expected 1 predicted 0.9819 which is correct\n",
            "for input [1, 0] expected 1 predicted 0.9819 which is correct\n",
            "for input [1, 1] expected 0 predicted 0.0181 which is correct\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sources**\n",
        "\n",
        "1. Building an RNN prediction:\n",
        "2. Ð¡hatgpt: https://chatgpt.com/share/a80ddd78-8384-43be-8af4-20c2187b6df1\n",
        "3. Neural Network Training Process: https://www.codingame.com/playgrounds/59631/neural-network-xor-example-from-scratch-no-libs\n",
        "4. Robotdreams: https://robotdreams.cc/uk/blog/327-funkciji-aktivaciji-stupinchasta-liniyna-sigmojida-relu-ta-tanh\n",
        "\n"
      ],
      "metadata": {
        "id": "tFtOiZrKOmf2"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rvIDyucT0OOV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0vlGLTQovYDV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}